{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SBS í‰ê°€: Denoising (í•™ìŠµ ëª¨ë¸) + Deconvolution (ìµœì†Œì œê³±ë²•)\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ Step-by-Step íŒŒì´í”„ë¼ì¸ì˜ ì¤‘ê°„ ì„±ëŠ¥ì„ í™•ì¸í•˜ê¸° ìœ„í•´ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "- **1ë‹¨ê³„ (Denoising)**: í•™ìŠµëœ DnCNN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n",
        "- **2ë‹¨ê³„ (Deconvolution)**: ê³ ì „ì ì¸ ì´ë¯¸ì§€ ë³µì› ê¸°ë²•ì¸ ìµœì†Œì œê³±ë²•(Least Squares)ì„ ì‚¬ìš©í•˜ì—¬ Deconvolutionì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì´ë¥¼ í†µí•´ ì „ì²´ SBS íŒŒì´í”„ë¼ì¸(DnCNN + Unet)ì˜ í•™ìŠµì´ ì™„ë£Œë˜ê¸° ì „ì— Denoising ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ í•œê³„ë¥¼ ì ì •ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 1. í™˜ê²½ ì„¤ì •\n",
        "# Google Drive ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# í”„ë¡œì íŠ¸ í´ë” ê²½ë¡œ ì„¤ì • (ë³¸ì¸ì˜ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)\n",
        "import os\n",
        "PROJECT_PATH = \"/content/drive/MyDrive/Data Scientist/Project/Week5/week5\"\n",
        "os.chdir(PROJECT_PATH)\n",
        "\n",
        "# ì‹œìŠ¤í…œ ê²½ë¡œì— í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€\n",
        "import sys\n",
        "sys.path.append(PROJECT_PATH)\n",
        "\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 2. ì„¤ì •ê°’ ì •ì˜\n",
        "# @markdown ---\n",
        "# @markdown ### **1. ì‹¤í–‰ ëª¨ë“œ ì„ íƒ**\n",
        "# @markdown - **Robust**: 5ê°œ ë°©í–¥ì„ ëª¨ë‘ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì´ë¯¸ì§€ë³„ ìµœì  ê²°ê³¼ë¥¼ ìë™ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤. (ê¶Œì¥)\n",
        "# @markdown - **Manual**: ì•„ë˜ì— ì§€ì •ëœ ë‹¨ì¼ ë°©í–¥ìœ¼ë¡œë§Œ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ë³µì›í•©ë‹ˆë‹¤. (ì‹¤í—˜ìš©)\n",
        "EVALUATION_MODE = \"Robust (All Directions)\" # @param [\"Robust (All Directions)\", \"Manual (Single Direction)\"]\n",
        "# @markdown ---\n",
        "# @markdown ### **2. í•„ìˆ˜ ê²½ë¡œ ì„¤ì •**\n",
        "# @markdown Denoising ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.\n",
        "DENOISING_CKPT_PATH = \"logs_sbs_denoising_dncnn/00001_train/checkpoints/checkpoint_best.ckpt\" # @param {type:\"string\"}\n",
        "# @markdown ---\n",
        "# @markdown ### **3. ë°ì´í„° ë° ê²°ê³¼ í´ë” ì„¤ì •**\n",
        "# @markdown í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ê³¼ ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•  í´ë”ë¥¼ ì§€ì •í•˜ì„¸ìš”.\n",
        "TEST_DATA_PATH = \"dataset/test_y\" # @param {type:\"string\"}\n",
        "RESULT_DIR = \"result_sbs_denoising_ls\" # @param {type:\"string\"}\n",
        "# @markdown ---\n",
        "# @markdown ### **4. ìµœì†Œì œê³±ë²• í•˜ì´í¼íŒŒë¼ë¯¸í„°**\n",
        "# @markdown Regularization ê°•ë„ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê°•í•œ ë³µì›, í´ìˆ˜ë¡ ë…¸ì´ì¦ˆ ì–µì œ)\n",
        "LAMBDA = 1e-3 # @param {type:\"number\"}\n",
        "# @markdown ---\n",
        "# @markdown ### **5. (Manual ëª¨ë“œ ì „ìš©) B0 ë°©í–¥ ìˆ˜ë™ ì„¤ì •**\n",
        "# @markdown `EVALUATION_MODE`ê°€ `Manual`ì¼ ë•Œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "B0_DIR_X = 0.309 # @param {type:\"number\"}\n",
        "B0_DIR_Y = -0.9511 # @param {type:\"number\"}\n",
        "MANUAL_B0_DIRECTION = (B0_DIR_X, B0_DIR_Y)\n",
        "# @markdown ---\n",
        "print(f\"âœ… Evaluation Mode: {EVALUATION_MODE}\")\n",
        "print(f\"Denoising Checkpoint: {DENOISING_CKPT_PATH}\")\n",
        "print(f\"Test Data: {TEST_DATA_PATH}\")\n",
        "print(f\"Result Directory: {RESULT_DIR}\")\n",
        "print(f\"Least Squares Lambda: {LAMBDA}\")\n",
        "if EVALUATION_MODE == 'Manual (Single Direction)':\n",
        "    print(f\"Manual B0 Direction: {MANUAL_B0_DIRECTION}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 3. ìµœì†Œì œê³±ë²•(Least Squares) ë³µì› í•¨ìˆ˜ êµ¬í˜„\n",
        "import torch\n",
        "import torch.fft as fft\n",
        "from dataset.forward_simulator import ForwardSimulator\n",
        "from params import config as global_config, conv_directions\n",
        "\n",
        "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ForwardSimulatorë¥¼ í•œ ë²ˆë§Œ ì´ˆê¸°í™”\n",
        "H, W = global_config.image_size\n",
        "forward_simulator = ForwardSimulator(image_dims=(H, W))\n",
        "\n",
        "# ğŸ’¡ ìˆ˜ì •: ì„ íƒëœ EVALUATION_MODEì— ë”°ë¼ ì»¤ë„ì„ ìƒì„±\n",
        "if EVALUATION_MODE == \"Robust (All Directions)\":\n",
        "    dipole_kernels_k = [\n",
        "        forward_simulator._create_dipole_kernel(B0_direction=direction).to(device)\n",
        "        for direction in conv_directions\n",
        "    ]\n",
        "    print(f\"âœ… Robust Mode: Successfully created {len(dipole_kernels_k)} dipole kernels.\")\n",
        "else: # Manual (Single Direction)\n",
        "    dipole_kernels_k = [\n",
        "        forward_simulator._create_dipole_kernel(B0_direction=MANUAL_B0_DIRECTION).to(device)\n",
        "    ]\n",
        "    print(f\"âœ… Manual Mode: Successfully created 1 dipole kernel for direction {MANUAL_B0_DIRECTION}.\")\n",
        "\n",
        "\n",
        "def least_squares_deconv(denoised_tensor: torch.Tensor, kernel_k: torch.Tensor, lambda_reg: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Performs Least Squares deconvolution in the Fourier domain.\n",
        "    \"\"\"\n",
        "    # ì…ë ¥ ì´ë¯¸ì§€ë¥¼ k-spaceë¡œ ë³€í™˜\n",
        "    denoised_k = fft.fftn(denoised_tensor, dim=(-2, -1))\n",
        "    \n",
        "    # ìµœì†Œì œê³±ë²• ê³µì‹ ì ìš©\n",
        "    kernel_k_conj = torch.conj(kernel_k)\n",
        "    kernel_mag_sq = torch.abs(kernel_k)**2\n",
        "    \n",
        "    numerator = kernel_k_conj * denoised_k\n",
        "    denominator = kernel_mag_sq + lambda_reg\n",
        "    \n",
        "    # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€\n",
        "    denominator[denominator == 0] = 1.0\n",
        "    \n",
        "    deconv_k = numerator / denominator\n",
        "    \n",
        "    # ë‹¤ì‹œ ì´ë¯¸ì§€ ê³µê°„ìœ¼ë¡œ ë³€í™˜\n",
        "    deconv_image = fft.ifftn(deconv_k, dim=(-2, -1))\n",
        "    \n",
        "    # ì‹¤ìˆ˜ë¶€ë§Œ ë°˜í™˜\n",
        "    return deconv_image.real\n",
        "\n",
        "print(\"Least Squares deconvolution function is ready.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 4. Denoising ëª¨ë¸ ë¡œë“œ\n",
        "from pathlib import Path\n",
        "from code_denoising.core_funcs import get_model, ModelType\n",
        "from params import dncnnconfig\n",
        "\n",
        "print(f\"Loading Denoising checkpoint from: {DENOISING_CKPT_PATH}\")\n",
        "\n",
        "try:\n",
        "    denoising_ckpt_path = Path(DENOISING_CKPT_PATH)\n",
        "    if not denoising_ckpt_path.exists():\n",
        "        raise FileNotFoundError(f\"Denoising checkpoint not found at {denoising_ckpt_path}\")\n",
        "\n",
        "    denoising_checkpoint = torch.load(denoising_ckpt_path, map_location=device)\n",
        "    \n",
        "    # ëª¨ë¸ íƒ€ì… ë° ì„¤ì • í• ë‹¹\n",
        "    global_config.model_type = denoising_checkpoint.get(\"model_type\", \"dncnn\")\n",
        "    global_config.model_config = dncnnconfig\n",
        "    \n",
        "    # ëª¨ë¸ ìƒì„± ë° state_dict ë¡œë“œ\n",
        "    denoising_network = get_model(global_config).to(device)\n",
        "    denoising_network.load_state_dict(denoising_checkpoint['model_state_dict'])\n",
        "    denoising_network.eval()\n",
        "    \n",
        "    print(f\"Successfully loaded Denoising model ({global_config.model_type}).\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the model: {e}\")\n",
        "    denoising_network = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 5. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° ê²°ê³¼ ì €ì¥\n",
        "import shutil\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from code_denoising.datawrapper.datawrapper import DataKey, get_data_wrapper_loader, LoaderConfig\n",
        "\n",
        "if denoising_network:\n",
        "    # --- ê¸°ì¡´ ê²°ê³¼ í´ë” ì‚­ì œ ---\n",
        "    result_path = Path(RESULT_DIR)\n",
        "    if result_path.exists():\n",
        "        print(f\"Removing old '{RESULT_DIR}' directory...\")\n",
        "        shutil.rmtree(result_path)\n",
        "    result_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # --- ë°ì´í„° ë¡œë” ì„¤ì • ---\n",
        "    loader_cfg: LoaderConfig = {\n",
        "        \"data_type\": global_config.data_type,\n",
        "        \"batch\": 8, # GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ ê°€ëŠ¥\n",
        "        \"num_workers\": 2,\n",
        "        \"shuffle\": False,\n",
        "        \"augmentation_mode\": 'none',\n",
        "        \"training_phase\": 'end_to_end',\n",
        "        \"noise_type\": global_config.noise_type,\n",
        "        \"noise_levels\": global_config.noise_levels,\n",
        "        \"conv_directions\": global_config.conv_directions\n",
        "    }\n",
        "    data_loader, _ = get_data_wrapper_loader(\n",
        "        file_path=[TEST_DATA_PATH],\n",
        "        training_mode=False,\n",
        "        data_wrapper_class='controlled',\n",
        "        **loader_cfg\n",
        "    )\n",
        "\n",
        "    if not data_loader:\n",
        "        print(f\"Failed to create data loader from {TEST_DATA_PATH}. No data found?\")\n",
        "    else:\n",
        "        print(f\"\\n[Phase 1/1] Creating result files from Denoising + Robust Least Squares pipeline...\")\n",
        "        with torch.no_grad():\n",
        "            for data in tqdm(data_loader, desc=\"Processing images\"):\n",
        "                image_noise = data[DataKey.image_noise].to(device)\n",
        "                filenames = data[DataKey.name]\n",
        "\n",
        "                # Step 1: Denoising (í•™ìŠµ ëª¨ë¸)\n",
        "                denoised_image_batch = denoising_network(image_noise)\n",
        "\n",
        "                # ğŸ’¡ ìˆ˜ì •: ë°°ì¹˜ ë‚´ ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ ìµœì  ë°©í–¥ íƒìƒ‰\n",
        "                for i in range(denoised_image_batch.shape[0]):\n",
        "                    single_denoised_tensor = denoised_image_batch[i:i+1]\n",
        "                    \n",
        "                    candidate_results = []\n",
        "                    candidate_variances = []\n",
        "\n",
        "                    # Step 2: 5ê°œ ì»¤ë„ ëª¨ë‘ì— ëŒ€í•´ Deconvolution ìˆ˜í–‰\n",
        "                    for kernel_k in dipole_kernels_k:\n",
        "                        deconv_result = least_squares_deconv(single_denoised_tensor, kernel_k, LAMBDA)\n",
        "                        candidate_results.append(deconv_result)\n",
        "                        candidate_variances.append(torch.var(deconv_result).item())\n",
        "                    \n",
        "                    # Step 3: ë¶„ì‚°ì´ ê°€ì¥ ë†’ì€ ê²°ê³¼ë¥¼ ìµœì í•´ë¡œ ì„ íƒ\n",
        "                    best_result_index = np.argmax(candidate_variances)\n",
        "                    final_image_tensor = candidate_results[best_result_index]\n",
        "\n",
        "                    # ê²°ê³¼ ì €ì¥\n",
        "                    pred_np = final_image_tensor.squeeze().cpu().numpy()\n",
        "                    base_filename = Path(filenames[i]).stem\n",
        "                    np.save(result_path / f\"{base_filename}.npy\", pred_np)\n",
        "        \n",
        "        print(f\"\\nFinished! Results are saved in '{RESULT_DIR}'.\")\n",
        "        print(\"You can now run the 'evaluate.ipynb' notebook to calculate the scores.\")\n",
        "else:\n",
        "    print(\"\\nSkipping pipeline execution because the model failed to load.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
