{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SBS í‰ê°€: Denoising (í•™ìŠµ ëª¨ë¸) + Deconvolution (ìµœì†Œì œê³±ë²•)\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ Step-by-Step íŒŒì´í”„ë¼ì¸ì˜ ì¤‘ê°„ ì„±ëŠ¥ì„ í™•ì¸í•˜ê¸° ìœ„í•´ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "- **1ë‹¨ê³„ (Denoising)**: í•™ìŠµëœ DnCNN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n",
        "- **2ë‹¨ê³„ (Deconvolution)**: ê³ ì „ì ì¸ ì´ë¯¸ì§€ ë³µì› ê¸°ë²•ì¸ ìµœì†Œì œê³±ë²•(Least Squares)ì„ ì‚¬ìš©í•˜ì—¬ Deconvolutionì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì´ë¥¼ í†µí•´ ì „ì²´ SBS íŒŒì´í”„ë¼ì¸(DnCNN + Unet)ì˜ í•™ìŠµì´ ì™„ë£Œë˜ê¸° ì „ì— Denoising ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ í•œê³„ë¥¼ ì ì •ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 1. í™˜ê²½ ì„¤ì •\n",
        "# Google Drive ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# í”„ë¡œì íŠ¸ í´ë” ê²½ë¡œ ì„¤ì • (ë³¸ì¸ì˜ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)\n",
        "import os\n",
        "PROJECT_PATH = \"/content/drive/MyDrive/Data Scientist/Project/Week5/week5\"\n",
        "os.chdir(PROJECT_PATH)\n",
        "\n",
        "# ì‹œìŠ¤í…œ ê²½ë¡œì— í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€\n",
        "import sys\n",
        "sys.path.append(PROJECT_PATH)\n",
        "\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title (Optional) 1-1. Install Dependencies\n",
        "# @markdown `loguru` ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì´ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.\n",
        "%pip install loguru --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 2. ì„¤ì •ê°’ ì •ì˜\n",
        "# @markdown ---\n",
        "# @markdown ### **1. í•„ìˆ˜ ê²½ë¡œ ì„¤ì •**\n",
        "# @markdown Denoising ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.\n",
        "DENOISING_CKPT_PATH = \"logs_sbs_denoising_dncnn/00001_train/checkpoints/checkpoint_best.ckpt\" # @param {type:\"string\"}\n",
        "# @markdown ---\n",
        "# @markdown ### **2. ë°ì´í„° ë° ê²°ê³¼ í´ë” ì„¤ì •**\n",
        "# @markdown í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ê³¼ ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•  í´ë”ë¥¼ ì§€ì •í•˜ì„¸ìš”.\n",
        "TEST_DATA_PATH = \"dataset/test_y\" # @param {type:\"string\"}\n",
        "RESULT_DIR = \"result_sbs_denoising_ls_joint\" # @param {type:\"string\"}\n",
        "# @markdown ---\n",
        "# @markdown ### **3. ê³µë™ ë³µì› í•˜ì´í¼íŒŒë¼ë¯¸í„°**\n",
        "# @markdown Regularization ê°•ë„ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê°•í•œ ë³µì›, í´ìˆ˜ë¡ ë…¸ì´ì¦ˆ ì–µì œ)\n",
        "LAMBDA = 1e-3 # @param {type:\"number\"}\n",
        "# @markdown ---\n",
        "\n",
        "print(f\"âœ… Joint Deconvolution (All Angles) Mode Enabled.\")\n",
        "print(f\"Denoising Checkpoint: {DENOISING_CKPT_PATH}\")\n",
        "print(f\"Test Data: {TEST_DATA_PATH}\")\n",
        "print(f\"Result Directory: {RESULT_DIR} (Joint)\")\n",
        "print(f\"Deconvolution Lambda: {LAMBDA}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 3. ë‹¤ê°ë„ ê³µë™ ë³µì›(Joint Deconvolution) í•¨ìˆ˜ êµ¬í˜„\n",
        "import torch\n",
        "import torch.fft as fft\n",
        "from dataset.forward_simulator import ForwardSimulator, dipole_kernel\n",
        "from params import config as global_config\n",
        "\n",
        "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ëª¨ë“  ë°©í–¥ì— ëŒ€í•œ Dipole Kernelì„ k-space ìƒì—ì„œ ë¯¸ë¦¬ ìƒì„±\n",
        "dipole_kernels_k = [\n",
        "    dipole_kernel(matrix_size=global_config.image_size, B0_dir=direction).to(device)\n",
        "    for direction in global_config.conv_directions\n",
        "]\n",
        "print(f\"âœ… Successfully created {len(dipole_kernels_k)} dipole kernels for joint deconvolution.\")\n",
        "\n",
        "\n",
        "def joint_deconv_all_angles(denoised_tensor: torch.Tensor, kernels_k: list[torch.Tensor], lambda_reg: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Performs joint deconvolution using all available angles (directions).\n",
        "    Implements the formula: X_hat = sum(H_conj * Y) / (sum(|H|^2) + lambda)\n",
        "    \"\"\"\n",
        "    # ì…ë ¥ ì´ë¯¸ì§€ë¥¼ k-spaceë¡œ ë³€í™˜\n",
        "    denoised_k = fft.fftn(denoised_tensor, dim=(-2, -1))\n",
        "\n",
        "    # ë¶„ì(numerator)ì™€ ë¶„ëª¨(denominator)ì˜ í•©ì„ ì €ì¥í•  í…ì„œ ì´ˆê¸°í™”\n",
        "    # torch.zeros_like()ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ shapeê³¼ deviceë¥¼ ê°–ë„ë¡ í•¨\n",
        "    numerator_sum = torch.zeros_like(denoised_k)\n",
        "    denominator_sum = torch.zeros_like(torch.abs(kernels_k[0])**2)\n",
        "    \n",
        "    # ê° ê°ë„(theta)ì— ëŒ€í•´ ë¶„ì/ë¶„ëª¨ë¥¼ ëˆ„ì \n",
        "    for kernel_k in kernels_k:\n",
        "        numerator_sum += torch.conj(kernel_k) * denoised_k\n",
        "        denominator_sum += torch.abs(kernel_k)**2\n",
        "        \n",
        "    # ìµœì¢… ìˆ˜ì‹ ì ìš©\n",
        "    denominator_sum += lambda_reg\n",
        "    \n",
        "    # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€\n",
        "    denominator_sum[denominator_sum == 0] = 1.0\n",
        "    \n",
        "    deconv_k = numerator_sum / denominator_sum\n",
        "    \n",
        "    # ë‹¤ì‹œ ì´ë¯¸ì§€ ê³µê°„ìœ¼ë¡œ ë³€í™˜\n",
        "    deconv_image = fft.ifftn(deconv_k, dim=(-2, -1))\n",
        "    \n",
        "    # ì‹¤ìˆ˜ë¶€ë§Œ ë°˜í™˜\n",
        "    return deconv_image.real\n",
        "\n",
        "print(\"Joint deconvolution function is ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 4. Denoising ëª¨ë¸ ë¡œë“œ\n",
        "from pathlib import Path\n",
        "from code_denoising.core_funcs import get_model, ModelType\n",
        "from params import config as global_config, dncnnconfig # ğŸ’¡ global_config ì¶”ê°€\n",
        "\n",
        "print(f\"Loading Denoising checkpoint from: {DENOISING_CKPT_PATH}\")\n",
        "\n",
        "try:\n",
        "    denoising_ckpt_path = Path(DENOISING_CKPT_PATH)\n",
        "    if not denoising_ckpt_path.exists():\n",
        "        raise FileNotFoundError(f\"Denoising checkpoint not found at {denoising_ckpt_path}\")\n",
        "\n",
        "    denoising_checkpoint = torch.load(denoising_ckpt_path, map_location=device)\n",
        "    \n",
        "    # ğŸ’¡ --- ëª¨ë¸ ì„¤ì • ë³µì› ë¡œì§ (create_evaluation_results.pyì™€ ë™ì¼í•˜ê²Œ) ---\n",
        "    # 1. ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ íƒ€ì… ë° ì„¤ì •ì„ ê°€ì ¸ì˜´\n",
        "    model_type_from_ckpt = denoising_checkpoint.get(\"model_type\", \"dncnn\") # ê¸°ë³¸ê°’ dncnn\n",
        "    model_config_from_ckpt = denoising_checkpoint.get('model_config')\n",
        "    \n",
        "    if model_config_from_ckpt:\n",
        "        print(\"Found model_config in checkpoint. Using it to build the model.\")\n",
        "        model_config = model_config_from_ckpt\n",
        "    else:\n",
        "        # 2. model_configê°€ ì—†ëŠ” êµ¬í˜• ì²´í¬í¬ì¸íŠ¸ì˜ ê²½ìš°, ê¸°ë³¸ dncnnconfig ì‚¬ìš©\n",
        "        print(\"model_config not found in checkpoint. Falling back to default dncnnconfig.\")\n",
        "        model_config = dncnnconfig\n",
        "    \n",
        "    # ëª¨ë¸ ìƒì„± ë° state_dict ë¡œë“œ\n",
        "    denoising_network = get_model(model_config, model_type_from_ckpt).to(device)\n",
        "    denoising_network.load_state_dict(denoising_checkpoint['model_state_dict'])\n",
        "    denoising_network.eval()\n",
        "    \n",
        "    print(f\"Successfully loaded Denoising model ({model_type_from_ckpt}).\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the model: {e}\")\n",
        "    denoising_network = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 5. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° ê²°ê³¼ ì €ì¥\n",
        "import shutil\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from code_denoising.datawrapper.datawrapper import DataKey, get_data_wrapper_loader, LoaderConfig\n",
        "\n",
        "if denoising_network:\n",
        "    # --- ê¸°ì¡´ ê²°ê³¼ í´ë” ì‚­ì œ ---\n",
        "    result_path = Path(RESULT_DIR)\n",
        "    if result_path.exists():\n",
        "        print(f\"Removing old '{RESULT_DIR}' directory...\")\n",
        "        shutil.rmtree(result_path)\n",
        "    result_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # --- ë°ì´í„° ë¡œë” ì„¤ì • ---\n",
        "    loader_cfg: LoaderConfig = {\n",
        "        \"data_type\": global_config.data_type,\n",
        "        \"batch\": 8, # GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ ê°€ëŠ¥\n",
        "        \"num_workers\": 2,\n",
        "        \"shuffle\": False,\n",
        "        \"augmentation_mode\": 'none',\n",
        "        \"training_phase\": 'end_to_end',\n",
        "        \"noise_type\": global_config.noise_type,\n",
        "        \"noise_levels\": global_config.noise_levels,\n",
        "        \"conv_directions\": global_config.conv_directions\n",
        "    }\n",
        "    data_loader, _ = get_data_wrapper_loader(\n",
        "        file_path=[TEST_DATA_PATH],\n",
        "        training_mode=False,\n",
        "        data_wrapper_class='controlled',\n",
        "        **loader_cfg\n",
        "    )\n",
        "\n",
        "    if not data_loader:\n",
        "        print(f\"Failed to create data loader from {TEST_DATA_PATH}. No data found?\")\n",
        "    else:\n",
        "        print(f\"\\n[Phase 1/1] Creating result files from Denoising + Joint Deconvolution pipeline...\")\n",
        "        with torch.no_grad():\n",
        "            for data in tqdm(data_loader, desc=\"Processing images\"):\n",
        "                image_noise = data[DataKey.image_noise].to(device)\n",
        "                filenames = data[DataKey.name]\n",
        "\n",
        "                # Step 1: Denoising (í•™ìŠµ ëª¨ë¸)\n",
        "                denoised_image_batch = denoising_network(image_noise)\n",
        "\n",
        "                # Step 2: Joint Deconvolution (ë‹¤ê°ë„ ê³µë™ ë³µì›)\n",
        "                # ë°°ì¹˜ ë‚´ ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ ë³µì› ìˆ˜í–‰\n",
        "                for i in range(denoised_image_batch.shape[0]):\n",
        "                    single_denoised_tensor = denoised_image_batch[i:i+1]\n",
        "                    \n",
        "                    # ìƒˆë¡œ êµ¬í˜„ëœ ê³µë™ ë³µì› í•¨ìˆ˜ í˜¸ì¶œ\n",
        "                    final_image_tensor = joint_deconv_all_angles(\n",
        "                        single_denoised_tensor, \n",
        "                        dipole_kernels_k, \n",
        "                        LAMBDA\n",
        "                    )\n",
        "\n",
        "                    # ê²°ê³¼ ì €ì¥\n",
        "                    pred_np = final_image_tensor.squeeze().cpu().numpy()\n",
        "                    base_filename = Path(filenames[i]).stem\n",
        "                    np.save(result_path / f\"{base_filename}.npy\", pred_np)\n",
        "        \n",
        "        print(f\"\\nFinished! Results are saved in '{RESULT_DIR}'.\")\n",
        "        print(\"You can now run the 'evaluate.ipynb' notebook to calculate the scores.\")\n",
        "else:\n",
        "    print(\"\\nSkipping pipeline execution because the model failed to load.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
