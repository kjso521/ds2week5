"""
ìµœì¢… í‰ê°€ë¥¼ ìœ„í•œ ê²°ê³¼ë¬¼(.npy íŒŒì¼) ìƒì„± ìŠ¤í¬ë¦½íŠ¸.

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í•™ìŠµëœ DnCNN ëª¨ë¸(.ckpt)ì„ ë¶ˆëŸ¬ì™€ `test_y` ë°ì´í„°ì…‹ì˜ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ë³µì›í•˜ê³ ,
`evaluate.ipynb`ê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹ì— ë§ì¶° ì§€ì •ëœ í´ë”ì— .npy íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""

import argparse
import os
from pathlib import Path
import sys
import warnings

# --- ì¤‘ìš”: ëª¨ë“  import ì´ì „ì— í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ ì‹œìŠ¤í…œ ê²½ë¡œì— ì¶”ê°€ ---
# ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ë˜ëŠ” ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, ìƒìœ„ 1ë‹¨ê³„ í´ë”(week5)ë¥¼ ê²½ë¡œì— ì¶”ê°€í•©ë‹ˆë‹¤.
sys.path.append(str(Path(__file__).resolve().parents[1]))

import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•„ìš”í•œ ëª¨ë“ˆ import
from code_denoising.datawrapper.datawrapper import DataKey, get_data_wrapper_loader, LoaderConfig
from code_denoising.core_funcs import get_model, ModelType
from params import config, dncnnconfig, unetconfig
from code_denoising.common.logger import logger

warnings.filterwarnings("ignore")

def create_sbs_results(
    denoising_network: torch.nn.Module,
    deconv_network: torch.nn.Module,
    data_loader: DataLoader,
    result_dir: str,
):
    """
    Generate and save model outputs from a step-by-step pipeline.
    """
    result_path = Path(result_dir)
    result_path.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"Saving SBS results to {result_path}")

    with torch.no_grad():
        for data in tqdm(data_loader, leave=False):
            image_noise = data[DataKey.image_noise].to(config.device)
            filenames = data[DataKey.name]

            # Step 1: Denoising
            denoised_image = denoising_network(image_noise)
            # Step 2: Deconvolution
            final_image = deconv_network(denoised_image)


            for i in range(final_image.shape[0]):
                pred_np = final_image[i, 0, :, :].cpu().numpy()
                filename = filenames[i]
                
                base_filename = Path(filename).stem
                
                np.save(result_path / f"{base_filename}.npy", pred_np)
    
    logger.info("Finished creating SBS result files.")


def main():
    parser = argparse.ArgumentParser(description="Create Step-by-Step evaluation results from two checkpoints.")
    parser.add_argument("--denoising_ckpt_path", type=str, required=True, help="Path to the Denoising model checkpoint file.")
    parser.add_argument("--deconv_ckpt_path", type=str, required=True, help="Path to the Deconvolution model checkpoint file.")
    parser.add_argument("--data_root", type=str, default="dataset/test_y", help="Path to the test data directory.")
    parser.add_argument("--result_dir", type=str, default="result_sbs", help="Directory to save the result .npy files.")
    args = parser.parse_args()

    # --- 1. Denoising ëª¨ë¸ ë¡œë“œ ---
    if not os.path.exists(args.denoising_ckpt_path):
        raise FileNotFoundError(f"Denoising checkpoint not found at {args.denoising_ckpt_path}")

    logger.info(f"Loading Denoising checkpoint from: {args.denoising_ckpt_path}")
    denoising_checkpoint = torch.load(args.denoising_ckpt_path, map_location=config.device)
    
    config.model_type = denoising_checkpoint.get("model_type", "dncnn")
    logger.info(f"Loading Denoising model type: {config.model_type}")

    # --- ğŸ’¡ ìˆ˜ì •: DnCNN ëª¨ë¸ ì„¤ì •ì„ configì— í• ë‹¹ ---
    if ModelType.from_string(config.model_type) == ModelType.DnCNN:
        config.model_config = dncnnconfig
    else:
        # SBSì˜ ì²« ë‹¨ê³„ëŠ” DnCNNì´ì–´ì•¼ í•˜ë¯€ë¡œ, ë‹¤ë¥¸ ëª¨ë¸ íƒ€ì…ì´ ì˜¤ë©´ ê²½ê³ 
        logger.warning(f"Expected DnCNN for denoising, but got {config.model_type}. Using default DnCNN config.")
        config.model_config = dncnnconfig
        
    denoising_network = get_model(config).to(config.device)
    
    denoising_state_dict = denoising_checkpoint['model_state_dict']
    if isinstance(denoising_network, torch.nn.DataParallel):
        denoising_network.module.load_state_dict(denoising_state_dict)
    else:
        denoising_network.load_state_dict(denoising_state_dict)
    denoising_network.eval()

    # --- 2. Deconvolution ëª¨ë¸ ë¡œë“œ ---
    if not os.path.exists(args.deconv_ckpt_path):
        raise FileNotFoundError(f"Deconvolution checkpoint not found at {args.deconv_ckpt_path}")

    logger.info(f"Loading Deconvolution checkpoint from: {args.deconv_ckpt_path}")
    deconv_checkpoint = torch.load(args.deconv_ckpt_path, map_location=config.device)

    config.model_type = deconv_checkpoint.get("model_type", "unet")
    logger.info(f"Loading Deconvolution model type: {config.model_type}")

    # --- ğŸ’¡ ìˆ˜ì •: Unet ëª¨ë¸ ì„¤ì •ì„ configì— í• ë‹¹ ---
    if ModelType.from_string(config.model_type) == ModelType.Unet:
        config.model_config = unetconfig
        # Deconvolution ëª¨ë¸ì€ í•­ìƒ 2ì±„ë„ ì…ì¶œë ¥ì„ ê°€ì§
        config.model_config.in_chans = 2
        config.model_config.out_chans = 2
    else:
        # SBSì˜ ë‘ ë²ˆì§¸ ë‹¨ê³„ëŠ” Unetì´ì–´ì•¼ í•˜ë¯€ë¡œ, ë‹¤ë¥¸ ëª¨ë¸ íƒ€ì…ì´ ì˜¤ë©´ ê²½ê³ 
        logger.warning(f"Expected Unet for deconvolution, but got {config.model_type}. Using default Unet config.")
        config.model_config = unetconfig
        config.model_config.in_chans = 2
        config.model_config.out_chans = 2

    deconv_network = get_model(config).to(config.device)

    deconv_state_dict = deconv_checkpoint['model_state_dict']
    if isinstance(deconv_network, torch.nn.DataParallel):
        deconv_network.module.load_state_dict(deconv_state_dict)
    else:
        deconv_network.load_state_dict(deconv_state_dict)
    deconv_network.eval()

    # --- ë°ì´í„° ë¡œë” ì„¤ì • ---
    # ğŸ’¡ ìˆ˜ì •: LoaderConfig í´ë˜ìŠ¤ ìƒì„± ëŒ€ì‹  TypedDict(ë”•ì…”ë„ˆë¦¬)ë¥¼ ì‚¬ìš©
    loader_cfg: LoaderConfig = {
        "data_type": config.data_type,
        "batch": 8,
        "num_workers": 0,
        "shuffle": False,
        "augmentation_mode": 'none',
        "training_phase": 'end_to_end', # ControlledDataWrapperì— í•„ìš”
        "noise_type": config.noise_type,
        "noise_levels": config.noise_levels,
        "conv_directions": config.conv_directions
    }
    # ğŸ’¡ ìˆ˜ì •: loader_cfgë¥¼ í‚¤ì›Œë“œ ì¸ìë¡œ í’€ì–´ì„œ ì „ë‹¬í•˜ê³ , 'controlled' í´ë˜ìŠ¤ ëª…ì‹œ
    data_loader, _ = get_data_wrapper_loader(
        file_path=[args.data_root],
        training_mode=False,
        data_wrapper_class='controlled',
        **loader_cfg
    )

    if not data_loader:
        logger.error(f"Failed to create data loader from {args.data_root}. No data found?")
        return
        
    # --- ê²°ê³¼ ìƒì„± ---
    create_sbs_results(denoising_network, deconv_network, data_loader, args.result_dir)

if __name__ == "__main__":
    main()
