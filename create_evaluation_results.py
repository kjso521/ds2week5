"""
ìµœì¢… í‰ê°€ë¥¼ ìœ„í•œ ê²°ê³¼ë¬¼(.npy íŒŒì¼) ìƒì„± ìŠ¤í¬ë¦½íŠ¸.

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í•™ìŠµëœ DnCNN ëª¨ë¸(.ckpt)ì„ ë¶ˆëŸ¬ì™€ `test_y` ë°ì´í„°ì…‹ì˜ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ë³µì›í•˜ê³ ,
`evaluate.ipynb`ê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹ì— ë§ì¶° ì§€ì •ëœ í´ë”ì— .npy íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""

import sys
from pathlib import Path
import warnings
import argparse
import os

import numpy as np
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm

# --- ì¤‘ìš”: ëª¨ë“  import ì´ì „ì— í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ ì‹œìŠ¤í…œ ê²½ë¡œì— ì¶”ê°€ ---
sys.path.append(str(Path(__file__).resolve().parents[1]))

from code_denoising.common.logger import logger
from code_denoising.core_funcs import get_model, ModelType
from code_denoising.datawrapper.datawrapper import BaseDataWrapper, DataKey, get_data_wrapper_loader, LoaderConfig
from code_denoising.common.utils import save_numpy_as_image
from params import config, parse_args_for_eval_script, unetconfig, dncnnconfig

warnings.filterwarnings("ignore")


def main():
    """
    Main function to run the evaluation.
    """
    # 1. Parse arguments and update the global config
    parse_args_for_eval_script()

    # 2. Setup paths and device
    checkpoint_path = Path(config.checkpoint_path)
    result_dir = Path(config.result_dir)
    result_dir.mkdir(parents=True, exist_ok=True)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load Checkpoint
    logger.info(f"Loading checkpoint from: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)

    # í‰ê°€ ì‹œì—ëŠ” augmentation_modeë¥¼ 'none'ìœ¼ë¡œ ê°•ì œí•˜ì—¬ ì±„ë„ ë¶ˆì¼ì¹˜ ë°©ì§€
    config.augmentation_mode = 'none'
    logger.info("Setting augmentation_mode to 'none' for evaluation.")

    # Get model type from checkpoint if not provided
    user_overrode_model_type = any(arg.startswith('--model_type') for arg in sys.argv)

    if user_overrode_model_type:
        # ì‚¬ìš©ìê°€ ì§ì ‘ ì§€ì •í–ˆë‹¤ë©´, parse_args_for_eval_scriptê°€ ì´ë¯¸ configì— ì„¤ì •í–ˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        logger.info(f"User explicitly specified model type: {config.model_type}")
    else:
        # ì‚¬ìš©ìê°€ ì§€ì •í•˜ì§€ ì•Šì•˜ë‹¤ë©´, ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì˜´
        logger.info("Inferring model type from checkpoint...")
        model_type_from_ckpt = checkpoint.get('model_type')
        if model_type_from_ckpt:
            config.model_type = model_type_from_ckpt
        else:
            # ì²´í¬í¬ì¸íŠ¸ì—ë„ ì •ë³´ê°€ ì—†ìœ¼ë©´, ë§ˆì§€ë§‰ ìˆ˜ë‹¨ìœ¼ë¡œ ê¸°ë³¸ê°’ì„ ì‚¬ìš© (ì´ ê²½ìš° configì˜ ê¸°ë³¸ê°’)
            logger.warning(f"Model type not found in checkpoint. Falling back to default: {config.model_type}")

    logger.info(f"Using model type: {config.model_type}")
    
    # ğŸ’¡ --- ëª¨ë¸ ì„¤ì • ë³µì› ë¡œì§ --- ğŸ’¡
    # 1. ì²´í¬í¬ì¸íŠ¸ì—ì„œ model_configë¥¼ ì§ì ‘ ê°€ì ¸ì˜¤ê¸° (ì‹ ê·œ ì²´í¬í¬ì¸íŠ¸ ë°©ì‹)
    model_config_from_ckpt = checkpoint.get('model_config')
    
    if model_config_from_ckpt:
        logger.info("Found model_config in checkpoint. Using it to build the model.")
        config.model_config = model_config_from_ckpt
    else:
        # 2. model_configê°€ ì—†ëŠ” ê²½ìš°, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì„¤ì • (í•˜ìœ„ í˜¸í™˜ì„±)
        logger.warning("model_config not found in checkpoint. Falling back to default config from params.py.")
        if ModelType.from_string(config.model_type) == ModelType.Unet:
            config.model_config = unetconfig
        elif ModelType.from_string(config.model_type) == ModelType.DnCNN:
            config.model_config = dncnnconfig

    # ğŸ’¡ --- ìˆ˜ë™ ì±„ë„ ì„¤ì • ë¡œì§ --- ğŸ’¡
    # ì‚¬ìš©ìê°€ --model_channels ì¸ìë¥¼ ì‚¬ìš©í–ˆë‹¤ë©´, ëª¨ë“  ì„¤ì •ì„ ë¬´ì‹œí•˜ê³  ì±„ë„ ìˆ˜ë¥¼ ê°•ì œë¡œ ë®ì–´ì“´ë‹¤.
    if config.model_channels_override is not None:
        logger.warning(f"User is manually overriding model channels to: {config.model_channels_override}")
        if ModelType.from_string(config.model_type) == ModelType.Unet:
            config.model_config.in_chans = config.model_channels_override
            config.model_config.out_chans = config.model_channels_override
        elif ModelType.from_string(config.model_type) == ModelType.DnCNN:
            config.model_config.channels = config.model_channels_override

    # ëª¨ë¸ ìƒì„±
    model = get_model(config.model_config, config.model_type).to(device)
    
    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸° (Size Mismatch ì˜¤ë¥˜ ë°©ì§€)
    try:
        model.load_state_dict(checkpoint['model_state_dict'])
    except RuntimeError as e:
        logger.error(f"Failed to load state_dict, likely due to a model architecture mismatch: {e}")
        logger.error("Please ensure the checkpoint was trained with a compatible architecture.")
        sys.exit(1) # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìŠ¤í¬ë¦½íŠ¸ ì¤‘ë‹¨
        
    model.eval()

    # 4. Setup data loader for the test dataset
    loader_cfg = {
        "data_type": '*.npy',
        "batch": 1,  # Process one image at a time
        "num_workers": 0,
        "shuffle": False,
        "augmentation_mode": 'none', # No augmentation during evaluation
        "training_phase": 'end_to_end', # This doesn't matter for eval but needs a value
        "noise_type": "gaussian",
        "noise_levels": [],
        "conv_directions": []
    }
    test_loader, _ = get_data_wrapper_loader(
        file_path=config.test_dataset,
        training_mode=False,
        data_wrapper_class='controlled',
        **loader_cfg
    )

    # 5. Run inference and save results
    logger.info(f"Starting inference on {len(test_loader.dataset)} images...")
    with torch.no_grad():
        for data in tqdm(test_loader):
            input_tensor = data[DataKey.image_noise].to(device)
            # íŒŒì¼ëª…ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ì˜¤ëŠ” ê²½ìš°ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ ì‚¬ìš©
            filename = data[DataKey.name][0] if isinstance(data[DataKey.name], list) else data[DataKey.name]

            # ğŸ’¡ --- ì…ë ¥ ì±„ë„ ë™ì  ë§ì¶¤ ë¡œì§ ---
            # ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì…ë ¥ ì±„ë„ ìˆ˜ë¥¼ í™•ì¸ (Unet: in_chans, DnCNN: channels)
            model_in_channels = getattr(config.model_config, 'channels', getattr(config.model_config, 'in_chans', 1))

            # ëª¨ë¸ì€ 2ì±„ë„ì„ ì›í•˜ëŠ”ë° ì…ë ¥ì´ 1ì±„ë„ì¸ ê²½ìš°, 0ìœ¼ë¡œ ì±„ì›Œì§„ ë‘ ë²ˆì§¸ ì±„ë„ì„ ì¶”ê°€
            if model_in_channels > 1 and input_tensor.shape[1] == 1:
                zeros = torch.zeros_like(input_tensor)
                input_tensor = torch.cat((input_tensor, zeros), dim=1)

            output_tensor = model(input_tensor)

            # ğŸ’¡ --- ì¶œë ¥ ì±„ë„ ë™ì  ë§ì¶¤ ë¡œì§ ---
            # ëª¨ë¸ ì¶œë ¥ì´ 2ì±„ë„(complex)ì¸ ê²½ìš°, í‰ê°€ë¥¼ ìœ„í•´ 1ì±„ë„ í¬ê¸°(magnitude)ë¡œ ë³€í™˜
            if output_tensor.shape[1] == 2:
                output_tensor = torch.sqrt(output_tensor[:, 0, :, :]**2 + output_tensor[:, 1, :, :]**2).unsqueeze(1)

            output_np = output_tensor.squeeze().cpu().numpy()
            save_path = result_dir / f"{Path(filename).stem}.npy"
            np.save(save_path, output_np)
    
    logger.info(f"Inference complete. Results saved to: {result_dir}")

if __name__ == "__main__":
    main()
    # Force reload in Colab environment
